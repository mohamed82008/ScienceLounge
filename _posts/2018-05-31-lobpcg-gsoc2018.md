---
title:  "My Eigen-Journey"
date:   2018-05-31
categories: text
---

<p>This blog post is about eigenvalues. If you are not on good terms with the eigen-family, this is a good time to reconcile. Happy Australian reconciliation week!</p>
<p>Eigenvalues are special mathematical properties that square matrices have. The eigenvalues of a <span class="math"><script type="math/tex">2 \times 2</script></span> matrix <span class="math"><script type="math/tex">A</script></span> say something about the extent of deformation the sides of a unit square of corners <span class="math"><script type="math/tex">\mathbf{p}_1</script></span> and <span class="math"><script type="math/tex">\mathbf{p}_2</script></span> would incur relative to each other, if you multiply <span class="math"><script type="math/tex">A</script></span> by the matrix <span class="math"><script type="math/tex">( \begin{smallmatrix} \mathbf{p}_1 & \mathbf{p}_2 \end{smallmatrix})</script></span> which could be <span class="math"><script type="math/tex">\bigl( \begin{smallmatrix} 1 & 0 \\ 0 & 1\end{smallmatrix} \bigr)</script></span> for example. An eigenvalue <span class="math"><script type="math/tex">\lambda</script></span> of the square matrix <span class="math"><script type="math/tex">A</script></span> and its corresponding eigenvector <span class="math"><script type="math/tex">x</script></span> satisfy the following mathematical relation:</p>
<span class="math"><script type="math/tex; mode=display">\mathbf{A} \mathbf{x} = \lambda \mathbf{x}
</script></span>
<p><em>Generalized</em> eigenvalues and eigenvectors are properties of square matrix pairs <span class="math"><script type="math/tex">\mathbf{A}</script></span> and <span class="math"><script type="math/tex">\mathbf{B}</script></span>, generalizing the above definition to:</p>
<span class="math"><script type="math/tex; mode=display">\mathbf{A} \mathbf{x} = \lambda \mathbf{B} \mathbf{x}
</script></span>
<p><em>Simple</em> eigenvalues and eigenvectors satsify the generalized condition where <span class="math"><script type="math/tex">\mathbf{B} = \mathbf{I}</script></span> is the identity matrix.</p>
<h2>Motivation from Structural Mechanics</h2>
<p>Given a certain loaded structure, one might want to ask: <em>what is the biggest number that I can multiply the current loads by without causing the structure to fall apart?</em> Generalized eigenvalues help us answer this question.</p>
<p>One straightforward way to answer the above question is to use some finite element software such as ABAQUS or ANSYS and compute the maximum von Mises stress <span class="math"><script type="math/tex">\sigma_v</script></span> that the current load configuration results in, then the answer would be <span class="math"><script type="math/tex">\sigma_y/\sigma_v</script></span> where <span class="math"><script type="math/tex">\sigma_y</script></span> is the yield stress of the material. This is because multiplying the loads by a certain factor roughly multiplies the stress by the same factor given certain conventional assumptions. While this answer is perfectly reasonable for a good number of structures, there is a problem with it, and it is not in the linear proportionality of stress and strain. The problem is that <strong>elastic strain is fundamentally nonlinear.</strong></p>
<h3>Rise of the quadratic terms</h3>
<p>Strain is a measure of deformation. For 1D structures such as a spring, <em>strain</em> is a linear function of how much the tip of the spring has moved. More specifically, it is the ratio of the displacement of the tip and the length of the unloaded spring. For 2D and 3D structures however, the strain, also known as the Green-Lagrange strain, is defined using the following formula:</p>
<span class="math"><script type="math/tex; mode=display">\epsilon_{ij} = \frac{1}{2}(v_{i,j} + v_{j,i} + v_{k,i}v_{k,j})
</script></span>
<p>where <span class="math"><script type="math/tex">v_i = v_i(\mathbf{p})</script></span> is the unit displacement along the <span class="math"><script type="math/tex">i^{th}</script></span> axis, of the point originally located at the position vector <span class="math"><script type="math/tex">\mathbf{p}</script></span>, when the structure was undeformed. <span class="math"><script type="math/tex">v_{i,j}(\mathbf{p})</script></span> is the rate of change of the function <span class="math"><script type="math/tex">v_i(\mathbf{p})</script></span> as the point <span class="math"><script type="math/tex">\mathbf{p}</script></span> moves along the <span class="math"><script type="math/tex">j^{th}</script></span> axis. The above definition uses tensor notation such that:</p>
<span class="math"><script type="math/tex; mode=display">v_{k,i}v_{k,j} := ∑_{k=1}^{dim}v_{k,i}v_{k,j}
</script></span>
<p>where <span class="math"><script type="math/tex">dim</script></span> is the dimension of the problem.</p>
<p>The above definition of strain is nonlinear in the displacement function’s derivatives. However typical linear stress analysis assumes the so-called <em>infinitesimal strain theory</em> which lets us drop the quadratic term from the above definition. This quadratic term can however come back and bite us in the neck leaving a mark that says <strong>Buckling</strong>.</p>
<p>Buckling happens when there exists a zero-energy deformation mode at a certain stress level. If a zero-energy deformation exists at the current state of the system, it is possible that subject to a finger flick, that the system may deform significantly along the zero-energy deformation mode vectors. Such can result in the phenonmenon known as buckling, and the system at this state is an unstable system.</p>
<p>Predicting when these zero-energy deformation modes are likely to show up is a challenging task. However, for the cases where we have a single load (or multiple treated as one), it is simple to answer the question: <em>what is the biggest number that I can multiply the current load by without reaching an unstable state?</em></p>
<p>To cut a long story short, taking the above quadratic terms into account and using certain assumptions, the smallest value for <span class="math"><script type="math/tex">\lambda</script></span> that satisfies the equation below for some <span class="math"><script type="math/tex">\mathbf{u}</script></span> is an estimate of the answer to the above question:</p>
<span class="math"><script type="math/tex; mode=display">\mathbf{K}\mathbf{u} = -\lambda \mathbf{K}_\sigma \mathbf{u}
</script></span>
<p>where <span class="math"><script type="math/tex">\mathbf{K}</script></span> is a symmetric positive definite (SPD) matrix derived from the material of the structure and its shape discretization, and <span class="math"><script type="math/tex">\mathbf{K}_\sigma</script></span> is a symmetric matrix derived from the shape discretization and stress distribution obtained using stress analysis.</p>
<p>The equation above maps closely to the generalized eigenvalue problem where <span class="math"><script type="math/tex">\mathbf{A} = \mathbf{K}</script></span> and <span class="math"><script type="math/tex">\mathbf{B} = -\mathbf{K}_\sigma</script></span>. The rest of this article will therefore be dedicated to finding the minimum (or maximum) eigenvalues of the system <span class="math"><script type="math/tex">\mathbf{A} \mathbf{x} = \lambda \mathbf{B} \mathbf{x}</script></span>.</p>
<h2>The locally optimal block preconditioned conjugate gradient (LOBPCG) method</h2>
<p>From now on, I will only be talking about symmetric matrices <span class="math"><script type="math/tex">\mathbf{A}</script></span> and <span class="math"><script type="math/tex">\mathbf{B}</script></span>. I will also assume that <span class="math"><script type="math/tex">\mathbf{B}</script></span> is positive definite. If <span class="math"><script type="math/tex">\mathbf{B}</script></span> is not positive definite but <span class="math"><script type="math/tex">A</script></span> is, then one can find the maximum eigenvalue of the system <span class="math"><script type="math/tex">\mathbf{B}x = \frac{1}{\lambda}\mathbf{A}\mathbf{x}</script></span> instead, then invert the eigenvalue. This will give us the minimum eigenvalue of <span class="math"><script type="math/tex">\mathbf{A} \mathbf{x} = \lambda \mathbf{B} \mathbf{x}</script></span>.</p>
<p>One way to find the minimum (maximum) eigenvalues of the system <span class="math"><script type="math/tex">\mathbf{A} \mathbf{x} = \lambda \mathbf{B} \mathbf{x}</script></span> is to minimize (maximize) the so-called Rayleigh quotient on the <span class="math"><script type="math/tex">\mathbf{B}</script></span>-ellipsoid. The Rayleigh quotient is defined as:</p>
<span class="math"><script type="math/tex; mode=display">\frac{\mathbf{x}'\mathbf{A}\mathbf{x}}{\mathbf{x}'\mathbf{B}\mathbf{x}}
</script></span>
<p>A vector <span class="math"><script type="math/tex">\mathbf{x}</script></span> lies on the <span class="math"><script type="math/tex">\mathbf{B}</script></span>-ellipsoid if it satisfies the condition <span class="math"><script type="math/tex">\mathbf{x}' \mathbf{B} \mathbf{x} == 1</script></span>. One might also be interested in finding the <span class="math"><script type="math/tex">k</script></span> smallest (largest) eigenvalues and their corresponding eigenvectors in which case the condition <span class="math"><script type="math/tex">\mathbf{X}'\mathbf{B}\mathbf{X}==\mathbf{I}_{k \times k}</script></span> needs to be satisfied, where the columns of <span class="math"><script type="math/tex">\mathbf{X}</script></span> are the eigenvectors corresponding to the <span class="math"><script type="math/tex">k</script></span> smallest (largest) eigenvalues. This condition means that any 2 eigenvectors <span class="math"><script type="math/tex">\mathbf{x}_1</script></span> and <span class="math"><script type="math/tex">\mathbf{x}_2</script></span> are <span class="math"><script type="math/tex">\mathbf{B}</script></span>-orthogonal, that is <span class="math"><script type="math/tex">\mathbf{x}_1' \mathbf{B} \mathbf{x}_2 == 0</script></span>. It also means that each eigenvector <span class="math"><script type="math/tex">\mathbf{x}</script></span> is <span class="math"><script type="math/tex">\mathbf{B}</script></span>-normalized, that is <span class="math"><script type="math/tex">\mathbf{x}' \mathbf{B} \mathbf{x} == 1</script></span>.</p>
<p>The LOBPCG algorithm uses an enhanced conjugate gradient (CG) method. The details of the algorithm will be left for another blog post. In the rest of this post, I will talk about my implementation challenges and progress as part of the Google Summer of Code (GSoC) with the NumFOCUS organization.</p>
<h2>LOBPCG.jl</h2>
<p>As part of my GSoC project, I am expected to implement the LOBPCG algorithm in the package <code style="font-family: Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;">IterativeSolvers.jl</code>. To make it easier to modify stuff and upload unfinished bugged code showcasing my progress, I created the <a href="https://github.com/mohamed82008/LOBPCG.jl">LOBPCG.jl</a> repository to host my code.</p>
<h3>Which algorithm is the LOBPCG?</h3>
<p>One challenge I had initially was to nail down one version of the algorithm to label it “LOBPCG”. For that and following a suggestion from my mentor Harmen Stoppels, I found the Python implementation of the algorithm by its inventor and a collaborator. The code was BSD licensed and since my translation to Julia would be considered <em>“derived work”</em>, I had to make an acknowledgement at the top of the file giving credit to the original authors and putting all necessary disclaimers that are part of the BSD license.</p>
<h3>Python -&gt; Julia -&gt; Fast Julia</h3>
<p>After reading the Python implementation, I translated it to Julia very naively, allocating a lot of memory unnecessarily, using type unstable code and using repetitive codes all over the place. It was a very close translation of the Python version so it had a lot of the Python features that makes it, well…slow! After making sure the translation is working, I refactored the code a bit to identify key elements of the program, it was a big mess with over 30 matrices interacting. This helped me identify the patterns in the code, which enabled my third refactoring pass. That third pass was the main pass. I used Julia’s callable structs feature which lets me group variables by the functions using them (pretty much the inverse of object-oriented programming). This made it possible to write clean, type-stable and modular code. I was careful to use <code style="font-family: Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;">view</code>s and in-place operations such as <code style="font-family: Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;">A_mul_B!</code>, pre-allocating all the necessary memory at the very beginning and simply resusing this memory later. The end result was a mostly in-place Julian version of the LOBPCG algorithm that doesn’t work! I had to fix a ton of bugs, often self-reflecting, and fantasizing about a world with no programming bugs, really none at all!!!</p>
<p>After a long patient tracking down of all the bugs, I was finally happy to see my program working. At that point the program was fast and mostly in-place. I then did a fourth refactoring pass removing most repetitive patterns in the code which resulted in the current version on Github. Preliminary tests show that the LOBPCG algorithm is about &gt;10x faster than the <code style="font-family: Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;">eigs</code> function of Julia for generalized eigenvalues, but is 5-10x slower for simple eigenvalues. Something is fishy though, since it seems that <code style="font-family: Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;">eigs</code> tries to factorize <code style="font-family: Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;">B</code> when solving the generalized eigenvalue problem. I will look further into this with Harmen.</p>
<h2>Next steps</h2>
<p>The next step will be to move my code <code style="font-family: Menlo, Consolas, &quot;DejaVu Sans Mono&quot;, monospace;">IterativeSolvers.jl</code> and devise more elaborate tests to benchmark my code including different versions in the future.</p>
